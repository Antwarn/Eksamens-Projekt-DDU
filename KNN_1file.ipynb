{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "data = [58, 59, 67, 53, 266, 0, 2004, 180, 66,\n",
        "        53, 56, 61, 50, 252, 0, 2004, 182, 76,\n",
        "        55, 56, 75, 51, 275, 0, 2004, 192, 90,\n",
        "        55, 56, 60, 50, 254, 0, 1998, 189, 84,\n",
        "        54, 59, 66, 50, 286, 0, 2001, 176, 72,\n",
        "        58, 55, 69, 54, 284, 0, 1999, 187, 73,\n",
        "        60, 59, 96, 55, 288, 0, 2005, 191, 97,\n",
        "        64, 63, 76, 54, 307, 0, 2006, 178, 76]\n",
        "\n",
        "\n",
        "\n",
        "# Konverter listen til et NumPy-array\n",
        "numpy_array = np.array(data)\n",
        "\n",
        "\n",
        "# Udskriv NumPy-arrayet\n",
        "print(numpy_array)\n",
        "\n",
        "\n",
        "# Define the number of features (excluding labels)\n",
        "num_features = 8\n",
        "\n",
        "\n",
        "# Reshape the data into a 2D array with multiple labels\n",
        "reshaped_data = numpy_array.reshape(-1, num_features + 1)  # +1 for the label\n",
        "\n",
        "\n",
        "# Split into X (features) and y (labels)\n",
        "X = reshaped_data[:, :-1]  # Features (all columns except the last)\n",
        "y = reshaped_data[:, -1]   # Labels (the last column)\n",
        "\n",
        "\n",
        "# Print X and y\n",
        "print(\"X (features):\")\n",
        "print(X)\n",
        "print(\"y (labels):\")\n",
        "print(y)\n",
        "\n",
        "\n",
        "\n",
        "# run knn on the data\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# divide data into training and testing\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "\n",
        "\n",
        "# train the model\n",
        "\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "# test the model\n",
        "\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "\n",
        "\n",
        "# print the accuracy\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "\n",
        "\n",
        "\n",
        "# draw accuracy vs k\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "k_range = range(1, 20)\n",
        "\n",
        "scores = []\n",
        "\n",
        "for k in k_range:\n",
        "\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = knn.predict(X_test)\n",
        "\n",
        "    scores.append(accuracy_score(y_test, y_pred))\n",
        "\n",
        "\n",
        "\n",
        "plt.plot(k_range, scores)\n",
        "\n",
        "plt.xlabel('k')\n",
        "\n",
        "plt.ylabel('accuracy')\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "id": "YwEZgyTK8LeD",
        "outputId": "3ebbb083-c090-4e7c-ad66-1c09cf7f04c4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  58   59   67   53  266    0 2004  180   66   53   56   61   50  252\n",
            "    0 2004  182   76   55   56   75   51  275    0 2004  192   90   55\n",
            "   56   60   50  254    0 1998  189   84   54   59   66   50  286    0\n",
            " 2001  176   72   58   55   69   54  284    0 1999  187   73   60   59\n",
            "   96   55  288    0 2005  191   97   64   63   76   54  307    0 2006\n",
            "  178   76]\n",
            "X (features):\n",
            "[[  58   59   67   53  266    0 2004  180]\n",
            " [  53   56   61   50  252    0 2004  182]\n",
            " [  55   56   75   51  275    0 2004  192]\n",
            " [  55   56   60   50  254    0 1998  189]\n",
            " [  54   59   66   50  286    0 2001  176]\n",
            " [  58   55   69   54  284    0 1999  187]\n",
            " [  60   59   96   55  288    0 2005  191]\n",
            " [  64   63   76   54  307    0 2006  178]]\n",
            "y (labels):\n",
            "[66 76 90 84 72 73 97 76]\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-7fb430c04977>\u001b[0m in \u001b[0;36m<cell line: 93>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_classification.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;31m# In that case, we do not need the distances to perform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;31m# the weighting so we do not compute them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0mneigh_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0mn_samples_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_samples_fit_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_neighbors\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mn_samples_fit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    811\u001b[0m                 \u001b[0;34m\"Expected n_neighbors <= n_samples, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m                 \u001b[0;34m\" but n_samples = %d, n_neighbors = %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_samples_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected n_neighbors <= n_samples,  but n_samples = 6, n_neighbors = 7"
          ]
        }
      ]
    }
  ]
}